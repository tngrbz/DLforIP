{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Inverse Problems - Summer 2024\n",
    "## Problem Set 4\n",
    "\n",
    "**Issued**: Tuesday, May 7, 2024, 1:00 pm\n",
    "\n",
    "**Due**: Tuesday, May 14, 2024, 1:00 pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Images with a U-Net\n",
    "\n",
    "In this homework, your task is to train a U-Net for image denoising using PyTorch. As dataset, we use the Berkeley Segmentation Dataset (BSDS300). This dataset contains 300 clean color images. For simplicity, we consider Gaussian noise and convert the images to grayscale (see below). The attached file `unet.py` contains a PyTorch implemenation of the U-Net for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from unet import Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset\n",
    "Running these commands in the terminal downloads the BSDS300 dataset as a `.tgz` file, unzip it and delete the `.tgz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\n",
    "#!tar -xvzf BSDS300-images.tgz\n",
    "#!rm BSDS300-images.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build PyTorch Datasets and Dataloaders\n",
    "\n",
    "We use all 200 images in `./BSDS300/images/train` for training. The first 50 images in `./BSDS300/images/test` are used for validation and the remaining 50 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./BSDS300\"\n",
    "\n",
    "train_set_dir = f\"{dataset_dir}/images/train\"\n",
    "train_img_files = [f\"{train_set_dir}/{filename}\" for filename in os.listdir(train_set_dir)]\n",
    "# use this to train with fewer data\n",
    "# train_img_files = random.sample(train_img_files, 50)\n",
    "\n",
    "test_set_dir = f\"{dataset_dir}/images/test\"\n",
    "test_img_files = [f\"{test_set_dir}/{filename}\" for filename in os.listdir(test_set_dir)]\n",
    "val_img_files = test_img_files[:50]\n",
    "test_img_files = test_img_files[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 + Solution\n",
    "We first implement a `torch.utils.data.Dataset` that returns pairs of noisy images and clean ground truth. \n",
    "\n",
    "Complete the code below so that the dataset returns images that are **grayscale** (BSDS contains RGB images, so you have to convert them) with pixel-range $[0,1]$. Add zero-mean Gaussian noise with variance `noise_var` to the clean images. To reduce computational cost, we also want to be able to use chunks instead of full-sized images for training. Therefore, implement code to split an image into non-overlapping chunks of size `(chunk_size, chunk_size)`. For example: An image of size `(512, 512)` should be split into 16 non-overlapping chunks of size `(128,128)`. If any of the image dimension is not divisible by `chunk_size`, simply crop the dimension until it is divisble by `chunk_size`. For reference: If you split all 200 train images into chunks of size `(128,128)`, you should end up with 1200 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from file, convert to grayscale and normalize to range [0,1]\n",
    "def load_img(file):\n",
    "    # open image as grayscale\n",
    "    img = Image.open(file).convert(\"L\")\n",
    "    img = torch.tensor(np.array(img))\n",
    "    # convert to range [0,1]\n",
    "    img = img / 255.\n",
    "    return img\n",
    "\n",
    "# split an image into chunks of size chunk_size x chunks_size (no padding, no overlap)\n",
    "def chunk_img(img, chunk_size):\n",
    "    chunks = img.unfold(0, chunk_size, chunk_size).unfold(1, chunk_size, chunk_size).reshape(-1, chunk_size, chunk_size)\n",
    "    return list(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyImageChunkDataset(Dataset):\n",
    "    def __init__(self, img_files, noise_var, chunk_size):\n",
    "        self.img_files = img_files\n",
    "        self.noise_var = noise_var\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunks_clean, self.chunk_noisy = self.get_clean_and_noisy_chunks()\n",
    "\n",
    "    def get_clean_and_noisy_chunks(self):\n",
    "        # load clean images\n",
    "        imgs_clean = [load_img(file) for file in self.img_files]\n",
    "        # split into chunks\n",
    "        chunks_clean = sum([chunk_img(img, chunk_size=self.chunk_size) for img in imgs_clean], [])\n",
    "        # add noise to chunks\n",
    "        chunks_noisy = [img + math.sqrt(self.noise_var) * torch.randn_like(img) for img in chunks_clean]\n",
    "        return chunks_clean, chunks_noisy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks_clean)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunk_noisy[idx], self.chunks_clean[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_var = 0.005  # more noise makes denoising harder, we suggest you keep this value\n",
    "train_chunk_size = 128  # depends on your hardware; we recommend 128\n",
    "\n",
    "train_set = NoisyImageChunkDataset(img_files=train_img_files, noise_var=noise_var, chunk_size=train_chunk_size)\n",
    "# for validation and testing, we do not have to split the images into chunks because we do not have to compute gradients\n",
    "# the images have shape (321, 481) or (481, 321) so we crop them to (321, 321) to facilitate data loading\n",
    "val_set = NoisyImageChunkDataset(img_files=val_img_files, noise_var=noise_var, chunk_size=321)\n",
    "test_set = NoisyImageChunkDataset(img_files=test_img_files, noise_var=noise_var, chunk_size=321)\n",
    "\n",
    "plt.imshow(val_set[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # depends on your hardware\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more pooling layers and convolutional kernels increase the complexity of the U-Net (see lecture notes)\n",
    "num_pool_layers = 2\n",
    "chans = 64\n",
    "device = \"cpu\"  # set to \"cuda\" or \"cuda:0\" if you have access to a GPU (e.g. via Google Colab)\n",
    "\n",
    "model = Unet(\n",
    "    in_chans=1,  # 1 input channel as we use grayscale images as input\n",
    "    out_chans=1,  # 1 output channel as the model returns grayscale images\n",
    "    num_pool_layers=num_pool_layers,\n",
    "    chans=chans\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 + Solution\n",
    "\n",
    "Choose a suitable loss for training and implement the trainig loop. Be sure to also check the validation loss (or PSNR) every few epochs to be sure that your model does not overfit.\n",
    "\n",
    "Hint: In deep learning it is often helpful to normalize the data before passing it through the model. In image-to-image tasks (such as denoising), we often also de-normalize the model output to map the pixels back into the original range. For example:\n",
    "````\n",
    "mean, std = mean(img_noisy), std(img_noisy)\n",
    "img_noisy = (img-mean) / std\n",
    "img_denoised = model(img_noisy)\n",
    "img_denoised = img_denoised * std + mean\n",
    "````\n",
    "An intuition behind this is that the model can then receive and return images in a fixed range which makes learning easier. Feel free to train your model with and without normalization to see if it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psnr(img1, img2) = 10 * log10( max_pixel_value^2 / mean_square_error(img1, img2) )\n",
    "# as we normalized all images to range [0,1], we have that max_pixel_value = 1 and the above formula reduces to\n",
    "# psnr(img1, img2) = -10 * log10( mean_square_error(img1, img2) )\n",
    "def get_psnr(gt, pred):\n",
    "    pred = pred.clamp(0, 1)  # clamp prediction pixel range to range [0,1]\n",
    "    mse = (gt-pred.clamp(0, 1)).pow(2).mean(dim=(-1,-2))\n",
    "    return -10 * torch.log10(mse)  # psnr reduces to this formula as we normlaized the images to range [0,1]\n",
    "\n",
    "def get_training_loss(imgs0, imgs1):\n",
    "    return (imgs0 - imgs1).pow(2).mean(dim=(-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # choose a suitable optimizer form torch.optim; we recommend to use the ADAM optimizer\n",
    "\n",
    "epochs = 10  # how many epochs to train\n",
    "check_val_every_epochs = 1\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for imgs_noisy, imgs_clean in tqdm.tqdm(train_loader, desc=f\"Training Epoch {e}\"):\n",
    "        imgs_noisy = imgs_noisy.to(device)\n",
    "        imgs_clean = imgs_clean.to(device)\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # normalize input\n",
    "        mean = imgs_noisy.mean(dim=(-1, -2), keepdim=True)\n",
    "        std = imgs_noisy.std(dim=(-1, -2), keepdim=True)\n",
    "        imgs_noisy = (imgs_noisy - mean) / std\n",
    "        # forward pass\n",
    "        imgs_denoised = model(imgs_noisy.unsqueeze(1)).squeeze(1)\n",
    "        # undo normalizaiton\n",
    "        imgs_denoised = imgs_denoised * std + mean\n",
    "        # find the Loss\n",
    "        loss = get_training_loss(imgs_clean, imgs_denoised).mean()\n",
    "        # calculate gradients \n",
    "        loss.backward()\n",
    "        # update Weights\n",
    "        optimizer.step()\n",
    "        # update epoch loss\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Training Loss: {train_loss}')\n",
    "    \n",
    "    if e % check_val_every_epochs == 0:\n",
    "        with torch.no_grad():\n",
    "            val_psnr = 0.0\n",
    "            for imgs_noisy, imgs_clean in tqdm.tqdm(val_loader, desc=f\"Validation Epoch {e}\"):\n",
    "                imgs_noisy = imgs_noisy.to(device)\n",
    "                imgs_clean = imgs_clean.to(device)\n",
    "                mean = imgs_noisy.mean(dim=(-1, -2), keepdim=True)\n",
    "                std = imgs_noisy.std(dim=(-1, -2), keepdim=True)\n",
    "                imgs_noisy = (imgs_noisy - mean) / std\n",
    "                imgs_denoised = model(imgs_noisy.unsqueeze(1)).squeeze(1)\n",
    "                imgs_denoised = imgs_denoised * std + mean\n",
    "                psnr = get_psnr(imgs_clean, imgs_denoised).mean()\n",
    "                val_psnr += psnr.item()\n",
    "            val_psnr /= len(val_loader)\n",
    "            print(f'Validation PSNR: {val_psnr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 + Solution\n",
    "Test your model on the `test_loader`. Use suitable image metrics (e.g. PSNR, SSIM) to quantitatively assess the denoising performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_psnr = 0.0\n",
    "    base_psnr = 0.0\n",
    "    for imgs_noisy, imgs_clean in tqdm.tqdm(test_loader, desc=\"Testing\"):\n",
    "        imgs_noisy = imgs_noisy.to(device)\n",
    "        imgs_clean = imgs_clean.to(device)\n",
    "        mean = imgs_noisy.mean(dim=(-1, -2), keepdim=True)\n",
    "        std = imgs_noisy.std(dim=(-1, -2), keepdim=True)\n",
    "        imgs_noisy = (imgs_noisy - mean) / std\n",
    "        imgs_denoised = model(imgs_noisy.unsqueeze(1)).squeeze(1)\n",
    "        imgs_denoised = (imgs_denoised * std) + mean\n",
    "        test_psnr += get_psnr(imgs_clean, imgs_denoised).mean().item()\n",
    "        base_psnr += get_psnr(imgs_clean, imgs_noisy).mean().item()\n",
    "    test_psnr /= len(test_loader)\n",
    "    base_psnr /= len(test_loader)\n",
    "    print(f\"PSNR of noisy images: {base_psnr}\")\n",
    "    print(f'PSNR of denoised images: {test_psnr}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('deepinverse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2a1b41b802cff5ec36e546562501ed1eeef0542a2e0896a37a0204af8235daf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
